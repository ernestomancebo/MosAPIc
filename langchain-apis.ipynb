{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs Langchain\n",
    "\n",
    "We we'll be doing some test evaluating:\n",
    "\n",
    "- Asking general questions to some API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import APIChain\n",
    "from langchain.chains.api.prompt import API_RESPONSE_PROMPT\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from langchain.llms import GPT4All\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the LLM. In this case we are loading GPT4All\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "993259it [22:39, 730.76it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import requests\n",
    "local_path = (\n",
    "    # replace with your desired local file path\n",
    "    \"./models/ggml-gpt4all-l13b-snoozy.bin\"\n",
    ")\n",
    "\n",
    "\n",
    "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Example model. Check https://github.com/nomic-ai/gpt4all for the latest models.\n",
    "url = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n",
    "\n",
    "# send a GET request to the URL to download the file. Stream since it's large\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# open the file in binary mode and write the contents of the response to it in chunks\n",
    "# This is a large file, so be prepared to wait.\n",
    "with open(local_path, 'wb') as f:\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "        if chunk:\n",
    "            f.write(chunk)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the GPT4All model is loaded into the `llm` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./models/ggml-gpt4all-l13b-snoozy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[71874]: Class GGMLMetalClass is implemented in both /Users/ernesto/work/github/MosAPIc/.venv/lib/python3.11/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x2b180c208) and /Users/ernesto/work/github/MosAPIc/.venv/lib/python3.11/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x2b1a58208). One of the two will be used. Which one is undefined.\n",
      "llama.cpp: loading model from ./models/ggml-gpt4all-l13b-snoozy.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 9807.47 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "llama.cpp: loading model from ./models/ggml-gpt4all-l13b-snoozy.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 9807.47 MB (+ 1608.00 MB per state)\n",
      "............................"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./models/ggml-gpt4all-l13b-snoozy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "........................................................................\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\n",
    "\n",
    "# If you want to use a custom model add the backend parameter\n",
    "# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
    "llm = GPT4All(model=local_path, backend=\"gptj\",\n",
    "              callbacks=callbacks, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the weather example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.api import open_meteo_docs\n",
    "\n",
    "chain_new = APIChain.from_llm_and_api_docs(\n",
    "    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      " https://api.open-meteo.com/v1/forecast?latitude=34.9025&longitude=-56.7875&hourly=[temperature_2m,snowfall,rain,showers,weathercode]&daily=[precipitation_amount,windspeed,winddirection,humidity,dewpoint,visibility,sunrise,sunset,solarnoon,solartime,moonset,moontime,phaseofthemoon,magneticfieldstrength,airpressure]&current_weather=true\n",
      "timeformat=iso8601\n",
      "timezone=GMT+0\u001b[32;1m\u001b[1;3m https://api.open-meteo.com/v1/forecast?latitude=34.9025&longitude=-56.7875&hourly=[temperature_2m,snowfall,rain,showers,weathercode]&daily=[precipitation_amount,windspeed,winddirection,humidity,dewpoint,visibility,sunrise,sunset,solarnoon,solartime,moonset,moontime,phaseofthemoon,magneticfieldstrength,airpressure]&current_weather=true\n",
      "timeformat=iso8601\n",
      "timezone=GMT+0\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{\"error\":true,\"reason\":\"Value of type 'Bool' required for key 'current_weather'.\"}\u001b[0m\n",
      " The error message indicates that a Boolean value was expected for the \"current\\_weather\" parameter, but it received an invalid input (most likely due to missing or incorrect data). Therefore, the API call did not return any useful information about the current weather in Uruguay at this time.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The error message indicates that a Boolean value was expected for the \"current\\\\_weather\" parameter, but it received an invalid input (most likely due to missing or incorrect data). Therefore, the API call did not return any useful information about the current weather in Uruguay at this time.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain_new.run('What is the weather like now in Costa Rica in celsius?')\n",
    "chain_new.run(\n",
    "    'What is the weather like right now in Uruguay in degrees Fahrenheit?')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
